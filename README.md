# Regularization-techniques-on-different-activation-functions

In this paper, I use Convolutional Neural Networks to envision which method to prevent overfitting works best with different activation functions in the short run. Though there has been many articles and reports about methods to prevent overfitting, I thought that they would vary in performance depending on which activation functions they are used with. The dataset is the MNIST handwritten digit database which contains 60,000 training samples and 10,000 testing samples. I train the models with 12 epochs each as more epochs are not necessary since the model converges to a reasonably high accuracy with just a few epochs and change the hyperparameters for every analysis I run. Finally, I graph the training and testing loss and accuracy curves for a 10% validation split from the training samples.

Some of the important regularization methods used are: ReLu, Dropout, L2, and L1.

To read and learn more about this project, please read the paper in the repository. The above is an excerpt from the paper.
